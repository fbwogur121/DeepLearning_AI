{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cde0c3",
   "metadata": {},
   "source": [
    "# 오늘은 LeNet 구조를 만들어봅시다\n",
    "\n",
    "\n",
    "LeNet 구조는 CNN이며, 초기에 만들어진 모델입니다. \n",
    "\n",
    "2가지 모델(Sigmoid, ReLU)를 만들어 두 모델의 성능을 비교해봅시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aff3fd",
   "metadata": {},
   "source": [
    "## 1.우선 필요 라이브러리를 import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd17ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bac6b",
   "metadata": {},
   "source": [
    "## 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9880ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.0.0  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5590af",
   "metadata": {},
   "source": [
    "## 3. MNIST 데이터 다운로드 \n",
    "\n",
    " 1. Training data와 Test data 분리하기\n",
    " \n",
    " 2. Training data를 Training data 와 Validation data로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00908077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 9912422/9912422 [00:00<00:00, 14492647.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 28881/28881 [00:00<00:00, 24935301.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████| 1648877/1648877 [00:00<00:00, 11771469.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 4542/4542 [00:00<00:00, 3253164.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "train_data = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('data', train=False, download=True, transform=transform)\n",
    "\n",
    "train, val = torch.utils.data.random_split(train_data, [50000, 10000])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ffb80",
   "metadata": {},
   "source": [
    "## 4. torch.nn을 이용하여 모델-1 만들기\n",
    "\n",
    "   1) 아래의 그림 중 LeNet 구조를 구현 할 것\n",
    "   \n",
    "   2) Sigmoid 활성화 함수를 이용할 것\n",
    "   \n",
    "   \n",
    "![](Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defacffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_1, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26eed9",
   "metadata": {},
   "source": [
    "## 5. torch.nn을 이용하여 모델-2 만들기\n",
    "\n",
    "   LeNet 모델에서 ReLU 활성화 함수를 사용하시요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ac70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_2, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de556825",
   "metadata": {},
   "source": [
    "## 7. 학습 준비하기\n",
    "\n",
    "1) 1 epoch를 학습할 수 있는 함수 만들기\n",
    "\n",
    "2) Test와 Validation data의 정확도 계산할 수 있는 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06030b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(train_loader, network, loss_func, optimizer, epoch):\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    log_interval = 300\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        # 미분값의 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagration 계산하기.\n",
    "        outputs = network(image)\n",
    "        \n",
    "        \n",
    "        # Cross_entropy 함수를 적용하여 loss를 구하고 저장하기\n",
    "        loss = loss_func(outputs, label)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # training accuracy 정확도 구하기 위해 맞는 샘플 개수 세기\n",
    "        pred = outputs.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "        # Gradinet 구하기\n",
    "        loss.backward()\n",
    "\n",
    "        # weight값 update 하기\n",
    "        optimizer.step()\n",
    "\n",
    "        # 학습 상황 출력\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'\n",
    "                  .format(epoch, batch_idx * len(label), len(train_loader.dataset),100. * batch_idx / len(train_loader),\n",
    "                          loss.item()))\n",
    "            \n",
    "    return train_losses, train_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c0dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(test_loader, network, loss_func, val = False):\n",
    "    correct = 0\n",
    "    \n",
    "    test_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, label) in enumerate(test_loader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            # Forward propagration 계산하기.\n",
    "            outputs = network(image)\n",
    "\n",
    "            # Cross_entropy 함수를 적용하여 loss를 구하기\n",
    "            loss = loss_func(outputs, label)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            # Batch 별로 정확도 구하기\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "        # 전체 정확도 구하기\n",
    "        test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "        #중간결과 출력\n",
    "        if val is True:\n",
    "                print('Validation set: Accuracy: {}/{} ({:.2f}%)\\n'\n",
    "              .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "        else:\n",
    "            print('Test set: Accuracy: {}/{} ({:.2f}%)\\n'\n",
    "                  .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    return test_losses, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d73c53",
   "metadata": {},
   "source": [
    "## 8. 위 정의된 함수로 학습 함수 만들기\n",
    "\n",
    "Adam Optimizer를 사용하여 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df29783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(network, learning_rate = 0.001):\n",
    "    \n",
    "    epoches = 15\n",
    "    \n",
    "    cls_loss = nn.CrossEntropyLoss()  # CrossEntropyLoss 함수 사용\n",
    "    optimizer = optim.Adam(network.parameters(), lr=learning_rate)  # Adam optimizer 사용\n",
    "    \n",
    "    train_losses_per_epoch = []\n",
    "    test_losses_per_epoch = []\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "                \n",
    "        # 모델를 학습 중이라고 선언하기\n",
    "        network.train()\n",
    "        \n",
    "        train_losses, train_correct = training_epoch(train_loader,network,cls_loss,optimizer, epoch)\n",
    "        \n",
    "        # epoch 별로 loss 평균값, 정확도 구하기\n",
    "        average_loss = np.mean(train_losses)\n",
    "        train_losses_per_epoch.append(average_loss)\n",
    "        \n",
    "        train_accuracy = train_correct / len(train_loader.dataset) * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # epoch 별로 정확도 출력\n",
    "        print('\\nTraining set: Accuracy: {}/{} ({:.2f}%)'\n",
    "              .format(train_correct, len(train_loader.dataset),100. * train_correct / len(train_loader.dataset)))\n",
    "\n",
    "        \n",
    "        ### 학습 중에 test 결과 보기\n",
    "        \n",
    "        # 모델 test 중인 것을 선언하기\n",
    "        network.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            test_losses, test_accuracy = test_epoch(val_loader, network, cls_loss, True)\n",
    "\n",
    "        test_losses_per_epoch.append(np.mean(test_losses))\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        test_losses, test_accuracy = test_epoch(test_loader, network, cls_loss, False)\n",
    "        \n",
    "    return train_losses_per_epoch, test_losses_per_epoch, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1394321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019125022 류재혁\n",
      "Train Epoch: 0 [0/50000 (0.00%)]\tLoss: 2.330729\n",
      "Train Epoch: 0 [19200/50000 (38.36%)]\tLoss: 0.203291\n",
      "Train Epoch: 0 [38400/50000 (76.73%)]\tLoss: 0.259508\n",
      "\n",
      "Training set: Accuracy: 44721/50000 (89.44%)\n",
      "Validation set: Accuracy: 9297/10000 (92.97%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0.00%)]\tLoss: 0.199514\n",
      "Train Epoch: 1 [19200/50000 (38.36%)]\tLoss: 0.208589\n",
      "Train Epoch: 1 [38400/50000 (76.73%)]\tLoss: 0.137953\n",
      "\n",
      "Training set: Accuracy: 47241/50000 (94.48%)\n",
      "Validation set: Accuracy: 9463/10000 (94.63%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0.00%)]\tLoss: 0.160542\n",
      "Train Epoch: 2 [19200/50000 (38.36%)]\tLoss: 0.207085\n",
      "Train Epoch: 2 [38400/50000 (76.73%)]\tLoss: 0.084850\n",
      "\n",
      "Training set: Accuracy: 48004/50000 (96.01%)\n",
      "Validation set: Accuracy: 9546/10000 (95.46%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0.00%)]\tLoss: 0.097750\n",
      "Train Epoch: 3 [19200/50000 (38.36%)]\tLoss: 0.044264\n",
      "Train Epoch: 3 [38400/50000 (76.73%)]\tLoss: 0.160948\n",
      "\n",
      "Training set: Accuracy: 48433/50000 (96.87%)\n",
      "Validation set: Accuracy: 9624/10000 (96.24%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0.00%)]\tLoss: 0.142417\n",
      "Train Epoch: 4 [19200/50000 (38.36%)]\tLoss: 0.094459\n",
      "Train Epoch: 4 [38400/50000 (76.73%)]\tLoss: 0.105798\n",
      "\n",
      "Training set: Accuracy: 48754/50000 (97.51%)\n",
      "Validation set: Accuracy: 9657/10000 (96.57%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0.00%)]\tLoss: 0.128529\n",
      "Train Epoch: 5 [19200/50000 (38.36%)]\tLoss: 0.022768\n",
      "Train Epoch: 5 [38400/50000 (76.73%)]\tLoss: 0.047572\n",
      "\n",
      "Training set: Accuracy: 49001/50000 (98.00%)\n",
      "Validation set: Accuracy: 9681/10000 (96.81%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0.00%)]\tLoss: 0.027800\n",
      "Train Epoch: 6 [19200/50000 (38.36%)]\tLoss: 0.026078\n",
      "Train Epoch: 6 [38400/50000 (76.73%)]\tLoss: 0.081742\n",
      "\n",
      "Training set: Accuracy: 49170/50000 (98.34%)\n",
      "Validation set: Accuracy: 9698/10000 (96.98%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0.00%)]\tLoss: 0.044180\n",
      "Train Epoch: 7 [19200/50000 (38.36%)]\tLoss: 0.032452\n",
      "Train Epoch: 7 [38400/50000 (76.73%)]\tLoss: 0.011957\n",
      "\n",
      "Training set: Accuracy: 49359/50000 (98.72%)\n",
      "Validation set: Accuracy: 9695/10000 (96.95%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0.00%)]\tLoss: 0.025827\n",
      "Train Epoch: 8 [19200/50000 (38.36%)]\tLoss: 0.010819\n",
      "Train Epoch: 8 [38400/50000 (76.73%)]\tLoss: 0.043579\n",
      "\n",
      "Training set: Accuracy: 49491/50000 (98.98%)\n",
      "Validation set: Accuracy: 9710/10000 (97.10%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0.00%)]\tLoss: 0.016719\n",
      "Train Epoch: 9 [19200/50000 (38.36%)]\tLoss: 0.018336\n",
      "Train Epoch: 9 [38400/50000 (76.73%)]\tLoss: 0.026033\n",
      "\n",
      "Training set: Accuracy: 49599/50000 (99.20%)\n",
      "Validation set: Accuracy: 9711/10000 (97.11%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0.00%)]\tLoss: 0.022046\n",
      "Train Epoch: 10 [19200/50000 (38.36%)]\tLoss: 0.032119\n",
      "Train Epoch: 10 [38400/50000 (76.73%)]\tLoss: 0.017973\n",
      "\n",
      "Training set: Accuracy: 49692/50000 (99.38%)\n",
      "Validation set: Accuracy: 9717/10000 (97.17%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0.00%)]\tLoss: 0.063533\n",
      "Train Epoch: 11 [19200/50000 (38.36%)]\tLoss: 0.008893\n",
      "Train Epoch: 11 [38400/50000 (76.73%)]\tLoss: 0.005208\n",
      "\n",
      "Training set: Accuracy: 49766/50000 (99.53%)\n",
      "Validation set: Accuracy: 9703/10000 (97.03%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0.00%)]\tLoss: 0.029192\n",
      "Train Epoch: 12 [19200/50000 (38.36%)]\tLoss: 0.015145\n",
      "Train Epoch: 12 [38400/50000 (76.73%)]\tLoss: 0.007075\n",
      "\n",
      "Training set: Accuracy: 49824/50000 (99.65%)\n",
      "Validation set: Accuracy: 9724/10000 (97.24%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0.00%)]\tLoss: 0.006341\n",
      "Train Epoch: 13 [19200/50000 (38.36%)]\tLoss: 0.019342\n",
      "Train Epoch: 13 [38400/50000 (76.73%)]\tLoss: 0.051756\n",
      "\n",
      "Training set: Accuracy: 49846/50000 (99.69%)\n",
      "Validation set: Accuracy: 9741/10000 (97.41%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0.00%)]\tLoss: 0.006180\n",
      "Train Epoch: 14 [19200/50000 (38.36%)]\tLoss: 0.009048\n",
      "Train Epoch: 14 [38400/50000 (76.73%)]\tLoss: 0.005411\n",
      "\n",
      "Training set: Accuracy: 49897/50000 (99.79%)\n",
      "Validation set: Accuracy: 9720/10000 (97.20%)\n",
      "\n",
      "Test set: Accuracy: 9753/10000 (97.53%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = Model_1().to(device)\n",
    "print(\"2019125022 류재혁\")\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64815daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019125022 류재혁\n",
      "Train Epoch: 0 [0/50000 (0.00%)]\tLoss: 2.335661\n",
      "Train Epoch: 0 [19200/50000 (38.36%)]\tLoss: 0.095989\n",
      "Train Epoch: 0 [38400/50000 (76.73%)]\tLoss: 0.136073\n",
      "\n",
      "Training set: Accuracy: 45859/50000 (91.72%)\n",
      "Validation set: Accuracy: 9457/10000 (94.57%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0.00%)]\tLoss: 0.171159\n",
      "Train Epoch: 1 [19200/50000 (38.36%)]\tLoss: 0.038019\n",
      "Train Epoch: 1 [38400/50000 (76.73%)]\tLoss: 0.055777\n",
      "\n",
      "Training set: Accuracy: 48177/50000 (96.35%)\n",
      "Validation set: Accuracy: 9650/10000 (96.50%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0.00%)]\tLoss: 0.238913\n",
      "Train Epoch: 2 [19200/50000 (38.36%)]\tLoss: 0.082533\n",
      "Train Epoch: 2 [38400/50000 (76.73%)]\tLoss: 0.136302\n",
      "\n",
      "Training set: Accuracy: 48734/50000 (97.47%)\n",
      "Validation set: Accuracy: 9661/10000 (96.61%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0.00%)]\tLoss: 0.026779\n",
      "Train Epoch: 3 [19200/50000 (38.36%)]\tLoss: 0.103897\n",
      "Train Epoch: 3 [38400/50000 (76.73%)]\tLoss: 0.047951\n",
      "\n",
      "Training set: Accuracy: 49028/50000 (98.06%)\n",
      "Validation set: Accuracy: 9676/10000 (96.76%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0.00%)]\tLoss: 0.048061\n",
      "Train Epoch: 4 [19200/50000 (38.36%)]\tLoss: 0.032334\n",
      "Train Epoch: 4 [38400/50000 (76.73%)]\tLoss: 0.043290\n",
      "\n",
      "Training set: Accuracy: 49230/50000 (98.46%)\n",
      "Validation set: Accuracy: 9704/10000 (97.04%)\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0.00%)]\tLoss: 0.028985\n",
      "Train Epoch: 5 [19200/50000 (38.36%)]\tLoss: 0.010393\n",
      "Train Epoch: 5 [38400/50000 (76.73%)]\tLoss: 0.015919\n",
      "\n",
      "Training set: Accuracy: 49378/50000 (98.76%)\n",
      "Validation set: Accuracy: 9708/10000 (97.08%)\n",
      "\n",
      "Train Epoch: 6 [0/50000 (0.00%)]\tLoss: 0.098825\n",
      "Train Epoch: 6 [19200/50000 (38.36%)]\tLoss: 0.023167\n",
      "Train Epoch: 6 [38400/50000 (76.73%)]\tLoss: 0.028381\n",
      "\n",
      "Training set: Accuracy: 49502/50000 (99.00%)\n",
      "Validation set: Accuracy: 9733/10000 (97.33%)\n",
      "\n",
      "Train Epoch: 7 [0/50000 (0.00%)]\tLoss: 0.005784\n",
      "Train Epoch: 7 [19200/50000 (38.36%)]\tLoss: 0.003088\n",
      "Train Epoch: 7 [38400/50000 (76.73%)]\tLoss: 0.013179\n",
      "\n",
      "Training set: Accuracy: 49553/50000 (99.11%)\n",
      "Validation set: Accuracy: 9716/10000 (97.16%)\n",
      "\n",
      "Train Epoch: 8 [0/50000 (0.00%)]\tLoss: 0.004081\n",
      "Train Epoch: 8 [19200/50000 (38.36%)]\tLoss: 0.056329\n",
      "Train Epoch: 8 [38400/50000 (76.73%)]\tLoss: 0.020116\n",
      "\n",
      "Training set: Accuracy: 49655/50000 (99.31%)\n",
      "Validation set: Accuracy: 9706/10000 (97.06%)\n",
      "\n",
      "Train Epoch: 9 [0/50000 (0.00%)]\tLoss: 0.061101\n",
      "Train Epoch: 9 [19200/50000 (38.36%)]\tLoss: 0.012967\n",
      "Train Epoch: 9 [38400/50000 (76.73%)]\tLoss: 0.006654\n",
      "\n",
      "Training set: Accuracy: 49673/50000 (99.35%)\n",
      "Validation set: Accuracy: 9748/10000 (97.48%)\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0.00%)]\tLoss: 0.005837\n",
      "Train Epoch: 10 [19200/50000 (38.36%)]\tLoss: 0.003357\n",
      "Train Epoch: 10 [38400/50000 (76.73%)]\tLoss: 0.005322\n",
      "\n",
      "Training set: Accuracy: 49718/50000 (99.44%)\n",
      "Validation set: Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0.00%)]\tLoss: 0.002001\n",
      "Train Epoch: 11 [19200/50000 (38.36%)]\tLoss: 0.010001\n",
      "Train Epoch: 11 [38400/50000 (76.73%)]\tLoss: 0.117119\n",
      "\n",
      "Training set: Accuracy: 49744/50000 (99.49%)\n",
      "Validation set: Accuracy: 9752/10000 (97.52%)\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0.00%)]\tLoss: 0.004810\n",
      "Train Epoch: 12 [19200/50000 (38.36%)]\tLoss: 0.008643\n",
      "Train Epoch: 12 [38400/50000 (76.73%)]\tLoss: 0.017931\n",
      "\n",
      "Training set: Accuracy: 49784/50000 (99.57%)\n",
      "Validation set: Accuracy: 9712/10000 (97.12%)\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0.00%)]\tLoss: 0.012953\n",
      "Train Epoch: 13 [19200/50000 (38.36%)]\tLoss: 0.003487\n",
      "Train Epoch: 13 [38400/50000 (76.73%)]\tLoss: 0.018069\n",
      "\n",
      "Training set: Accuracy: 49837/50000 (99.67%)\n",
      "Validation set: Accuracy: 9754/10000 (97.54%)\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0.00%)]\tLoss: 0.000807\n",
      "Train Epoch: 14 [19200/50000 (38.36%)]\tLoss: 0.001262\n",
      "Train Epoch: 14 [38400/50000 (76.73%)]\tLoss: 0.000782\n",
      "\n",
      "Training set: Accuracy: 49796/50000 (99.59%)\n",
      "Validation set: Accuracy: 9749/10000 (97.49%)\n",
      "\n",
      "Test set: Accuracy: 9757/10000 (97.57%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = Model_2().to(device)\n",
    "print(\"2019125022 류재혁\")\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b1471",
   "metadata": {},
   "source": [
    "## 9. 두모델의 성능을 비교하시오"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8156f",
   "metadata": {},
   "source": [
    "정답)<br>\n",
    "Sigmoid 함수의 정확도는 Accuracy: 9753/10000 (97.53%)이 나왔고<br>\n",
    "ReLU 함수의 정확도는 Accuracy: 9757/10000 (97.57%)이 나왔다.<br>\n",
    "따라서 ReLU함수가 성능이 더 좋다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e7cced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019125022 류재혁\n"
     ]
    }
   ],
   "source": [
    "print(\"2019125022 류재혁\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
